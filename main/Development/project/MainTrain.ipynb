{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d937a913-618f-423d-8c6d-4ea4c55aba44",
   "metadata": {},
   "source": [
    "# Jupyter notebook for handling training of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764ec9a3-a4a1-44bc-9a25-6368a7f12660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import nbimporter\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644baa57-2e79-4645-9cac-b4483a89ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import HelperFunctions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f06b04-63b6-4be6-bdf2-8e5b7e87aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HelperFunctions\n",
    "reload(HelperFunctions)\n",
    "import HelperFunctions as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410461f-6368-4c86-a8c0-ff34538892b8",
   "metadata": {},
   "source": [
    "## Continue with previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39827c13-359b-410d-bbff-9a68a326884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "parameters stored\n",
      "model type initialized: VisualTransformer  \n",
      "Optimizer: SGD\n",
      "loss function: CrossEntropyLoss\n",
      "VisualTransformer(\n",
      "  (patch_embed): PatchEmbedding(\n",
      "    (projection): Conv2d(1, 64, kernel_size=(7, 7), stride=(7, 7))\n",
      "  )\n",
      "  (transformer): Sequential(\n",
      "    (0): TransformerEncoderBlock(\n",
      "      (attn): MultiHeadSelfAttention(\n",
      "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerEncoderBlock(\n",
      "      (attn): MultiHeadSelfAttention(\n",
      "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerEncoderBlock(\n",
      "      (attn): MultiHeadSelfAttention(\n",
      "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerEncoderBlock(\n",
      "      (attn): MultiHeadSelfAttention(\n",
      "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerEncoderBlock(\n",
      "      (attn): MultiHeadSelfAttention(\n",
      "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerEncoderBlock(\n",
      "      (attn): MultiHeadSelfAttention(\n",
      "        (out_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (q_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Load complete\n"
     ]
    }
   ],
   "source": [
    "#List of moodels\n",
    "previous_models = [\n",
    "#0\n",
    "    \"NeuralNetwork001.pth\",  \n",
    "#1\n",
    "    \"NeuralNetwork002.pth\",\n",
    "#2\n",
    "    \"CONVNet001.pth\",\n",
    "\n",
    "\n",
    "#3    \n",
    "    \"ClassifierOfEncodedData001.pth\",\n",
    "#4\n",
    "    \"SimpleAutoencoder001.pth\",\n",
    "\n",
    "\n",
    "    \n",
    "#5\n",
    "    \"VisualTransformer001.pth\",\n",
    "#6\n",
    "    \"VisualTransformerSevenPatchSize001.pth\",\n",
    "#7\n",
    "    \"VisualTransformerTest001.pth\",\n",
    "\n",
    "\n",
    "#8\n",
    "    \"AltVisualTransformerGELU001.pth\",\n",
    "#9\n",
    "    \"AltVisualTransformerGELUNoPathchAdam001.pth\",\n",
    "#10\n",
    "    \"AltVisualTransformerReLUNoPathchSGD001.pth\",\n",
    "\n",
    "\n",
    "\n",
    "#11    \n",
    "    \"ShapeTransformerBigSDG001.pth\",\n",
    "#12\n",
    "    \"ShapeTransformerSmallSDG001.pth\",\n",
    "]\n",
    "    \n",
    "\n",
    "CMM = hf.load_model_manager(previous_models[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "015791c8-82b0-47df-9064-0a764a1533e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard dataset:\n",
    "standard_train = hf.get_MNIST_train_data(hf.get_standard_MNIST_training_transform())\n",
    "standard_test  = hf.get_MNIST_test_data(hf.get_standard_MNIST_test_transform())\n",
    "\n",
    "#Test DataLoader:\n",
    "train_dataloader = hf.set_dataloader(standard_train , hf.standard_batch_size_64())\n",
    "test_dataloader = hf.set_dataloader(standard_test, hf.standard_batch_size_64(), shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f339577f-9dd7-4630-bf39-257d3b46d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch count 56\n",
      "----------------------------------\n",
      "loss: 0.252454  [   64/60000]\n",
      "loss: 0.233130  [ 6464/60000]\n",
      "loss: 0.190970  [12864/60000]\n",
      "loss: 0.107002  [19264/60000]\n",
      "loss: 0.222752  [25664/60000]\n",
      "loss: 0.047745  [32064/60000]\n",
      "loss: 0.328671  [38464/60000]\n",
      "loss: 0.126505  [44864/60000]\n",
      "loss: 0.203351  [51264/60000]\n",
      "loss: 0.215143  [57664/60000]\n",
      "Execution time of epoch: 40.26 seconds\n",
      "Test Error of epoch 56: \n",
      " Accuracy: 96.0%, Avg loss: 0.129063 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "alert(\"Task completed!\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = CMM.initiate_training( 1, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e459428-a73e-49dc-840e-3d102060992f",
   "metadata": {},
   "outputs": [],
   "source": [
    " hf.save_model_manager(CMM, \"001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ec461-6495-4f81-ad6f-0cb1ecdc95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shape data for transformer\n",
    "raw_train = hf.get_raw_MNIST_train_data()\n",
    "raw_test  = hf.get_raw_MNIST_test_data()\n",
    "# Convert entire dataset\n",
    "converted_MNIST_train = hf.convert_and_optimize_dataset(raw_train , optimize=False) \n",
    "converted_MNIST_test =  hf.convert_and_optimize_dataset(raw_test ,  optimize=False) \n",
    "\n",
    "train_dataloader = hf.set_dataloader( converted_MNIST_train , hf.standard_batch_size_64() , collate_fn = hf.collate_fn_padded)\n",
    "test_dataloader = hf.set_dataloader(converted_MNIST_test , hf.standard_batch_size_64()  , shuffle = False, collate_fn = hf.collate_fn_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb25f2-7f3d-4863-81e8-384141235dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load shape data for autoencoder OLd code, not tested\n",
    "\n",
    "\n",
    "\n",
    "# train_data_raw = hf.get_raw_MNIST_train_data()\n",
    "# test_data_raw  = hf.get_raw_MNIST_test_data()\n",
    "\n",
    "# #transforming data to basic shape based inputs!\n",
    "\n",
    "# # Create empty datasets for training and testing\n",
    "# train_data = np.zeros((60000, 512), dtype=int)  # 60000 samples for training, 512 grid types\n",
    "# train_labels = np.zeros(60000, dtype=int)       # Labels for the training set\n",
    "\n",
    "# test_data = np.zeros((10000, 512), dtype=int)   # 10000 samples for testing, 512 grid types\n",
    "# test_labels = np.zeros(10000, dtype=int)        # Labels for the test set\n",
    "\n",
    "# threshold = deault_threshold_value_0_2()\n",
    "\n",
    "# # Process the training dataset\n",
    "# for i, (image, label) in enumerate(train_data_raw):\n",
    "#     image = image.squeeze(0).numpy()  # Remove the batch dimension (1, 28, 28) -> (28, 28)\n",
    "    \n",
    "#     grid_counts = np.zeros(512)  # 512 possible 3x3 grid classifications\n",
    "    \n",
    "#     for x in range(26):  \n",
    "#         for y in range(26):\n",
    "#             # Extract the 3x3 grid\n",
    "#             grid = image[x:x+3, y:y+3]\n",
    "#             grid_class = classification_3x3_to_shape(grid, threshold) \n",
    "#             grid_counts[grid_class  ] += 1\n",
    "    \n",
    "#     # Store the grid representation (entire grid count) in the dataset\n",
    "#     train_data[i] = grid_counts\n",
    "#     train_labels[i] = label  # Store the label\n",
    "\n",
    "# # Process the testing dataset\n",
    "# for i, (image, label) in enumerate(test_data_raw):\n",
    "#     image = image.squeeze(0).numpy()  # Remove the batch dimension (1, 28, 28) -> (28, 28)\n",
    "    \n",
    "#     # Loop through the image with a 3x3 sliding window\n",
    "#     grid_counts = np.zeros(512)  # 512 possible 3x3 grid classifications\n",
    "    \n",
    "#     for x in range(26):  # 28 - 3 + 1 = 26 (possible 3x3 windows)\n",
    "#         for y in range(26):\n",
    "#             # Extract the 3x3 grid\n",
    "#             grid = image[x:x+3, y:y+3]\n",
    "#             # Get the binary classification of this grid\n",
    "#             grid_class = classification_3x3_to_shape(grid, threshold)\n",
    "#             grid_counts[grid_class] += 1\n",
    "    \n",
    "#     # Store the grid representation (entire grid count) in the dataset\n",
    "#     test_data[i] = grid_counts\n",
    "#     test_labels[i] = label  # Store the label\n",
    "\n",
    "# # Convert the data to tensors\n",
    "# train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "# train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)  # Labels should be long for classification\n",
    "\n",
    "# test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "# test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)  # Labels should be long for classification\n",
    "\n",
    "# # Create TensorDatasets\n",
    "# train_dataset_tensor = torch.utils.data.TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "# test_dataset_tensor = torch.utils.data.TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "\n",
    "# # Create DataLoaders for shape based inputs\n",
    "# batch_size = standard_batch_size_64()\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset_tensor , standard_batch_size_64(), shuffle=True)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset_tensor , standard_batch_size_64() , shuffle=False)\n",
    "\n",
    "# # train_loader = DataLoader(train_dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "# # test_loader = DataLoader(test_dataset_tensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Example: Iterate over batches in the train_loader\n",
    "# for batch_data, batch_labels in train_dataloader:\n",
    "#     print(batch_data.shape)  # Will print the shape of each batch (e.g., [64, 512])\n",
    "#     print(batch_labels.shape)  # Will print the shape of each label batch (e.g., [64])\n",
    "#     break  # Just print one batch as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e9efc-494e-404c-b08f-8bea18b3ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the data\n",
    "# with open(\"SimpleShapeData_train_data.json\", \"w\") as file:\n",
    "#     json.dump(train_data.tolist(), file, indent=2)  \n",
    "# with open(\"SimpleShapeData_train_labels.json\", \"w\") as file:\n",
    "#     json.dump(train_labels.tolist(), file, indent=2)  \n",
    "# with open(\"SimpleShapeData_test_data.json\", \"w\") as file:\n",
    "#     json.dump(test_data.tolist(), file, indent=2)  \n",
    "# with open(\"SimpleShapeData_test_labels.json\", \"w\") as file:\n",
    "#     json.dump(test_labels.tolist(), file, indent=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81f063-3799-4e6c-9ce4-40608807ee29",
   "metadata": {},
   "source": [
    "## Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f20a4d-171f-44d1-b558-cb5d3f4edf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class  = [\n",
    "#0\n",
    "    \"NeuralNetwork\",  \n",
    "#1\n",
    "   \"ShapeBasedNeuralNetwork\",\n",
    "#2\n",
    "    \"CONVNet\",\n",
    "#3    \n",
    "    \"ClassifierOfEncodedData\",\n",
    "#4\n",
    "    \"SimpleAutoencoder\",\n",
    "#5\n",
    "    \"VisualTransformer\",\n",
    "#8\n",
    "    \"AltVisualTransformer\",\n",
    "#11    \n",
    "    \"ShapeVisualTransformer\",\n",
    "#12\n",
    "    \"ShapeTransformer\",\n",
    "]\n",
    "    \n",
    "loss_types = [\n",
    "    \"CrossEntropyLoss\",\n",
    "    \"MSELoss\",\n",
    "    \n",
    "]\n",
    "\n",
    "optimizer_types = [\n",
    "    \"SGD\",\n",
    "    \"Adam\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa271dbd-7a4e-49b5-9439-16779499ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMM =  hf.model_manager( model_class = \"NeuralNetwork\", \n",
    "                     loss_function_name = \"CrossEntropyLoss\",\n",
    "                     optimizer_name = \"SGD\",\n",
    "                     optimizer_params = {\"lr\": 0.001},)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
