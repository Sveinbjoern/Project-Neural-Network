{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "31192263-62a0-40c5-89cc-4321185329bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45c023-ad06-43ab-8a09-ad401855d0a8",
   "metadata": {},
   "source": [
    "# Helper functions, contains code that is used multiple times in the project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ac6d4-0d23-41da-afd6-567c5a371b78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "70164397-7a12-420a-97c3-9e9f928b6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "10250d1a-4e3d-42ef-ba12-1f1af676f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7e21dfb0-af50-432d-aa9c-d65c6c49a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from importlib import reload\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58123c0-a225-410b-9cb2-0bfe1417b9b7",
   "metadata": {},
   "source": [
    "### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "48aa3bc1-3a5d-45a7-8ac7-64253d1d3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyModels import create_empty_model\n",
    "from MyModels import create_loss_function\n",
    "from MyModels import create_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2f8fb3b0-6a18-4b86-94a4-e5607c47c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MyModels\n",
    "reload(MyModels)\n",
    "from MyModels import create_empty_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9fb7a-c4f2-442f-8808-1489abd182d7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b9740d94-5487-48bf-8b50-ad1085959e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tellALie():\n",
    "    \"\"\"Test if the helperFunctions.ipynb or hf library has loaded successfully, but still lies to you!\"\"\"\n",
    "    print(\"You have all the time in the world!\\nYour helperfunctions library has been loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3eeda-5788-4df3-87c8-22d96b3018ae",
   "metadata": {},
   "source": [
    "## Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3ac271f2-8574-43e0-88af-bcf97ff06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_manager():\n",
    "    \"\"\"\n",
    "    Model managers gathers everything with testing and training in one class!\n",
    "    To create a new model manager you need: model_class, loss_function_name ,optimizer_name,optimizer_params. \n",
    "    Has default values for: model_parameters = {},  hidden_layers = 0, layer_sizes = [], singel_outputs = False, \n",
    "    stats = []\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_class, loss_function_name ,optimizer_name,optimizer_params , \n",
    "                 model_parameters = None,  hidden_layers = 0, \n",
    "                 layer_sizes = None, singel_outputs = False, stats = None):\n",
    "        \"\"\" \n",
    "        Needs: model_class, loss_function_name ,optimizer_name,optimizer_params. \n",
    "        Has default values for: model_parameters = {},  hidden_layers = 0, layer_sizes = [], singel_outputs = False,  stats = [] \n",
    "        \"\"\"\n",
    "        \n",
    "        # avoid issues with parameters being created at initialization by exlicitly making the None then create emrty mutables\n",
    "        if model_parameters == None: model_parameters = {}\n",
    "        if stats == None:            stats = []\n",
    "        if layer_sizes == None:      layer_sizes = []\n",
    "        \n",
    "        \n",
    "        # Set parameters\n",
    "        self.device = get_device()\n",
    "        self.model_class = model_class\n",
    "        self.model_parameters = model_parameters\n",
    "        self.loss_function_name = loss_function_name  \n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.singel_outputs = singel_outputs\n",
    "        self.num_epochs = len(stats)\n",
    "        self.stats = stats\n",
    "        \n",
    "        print(\"parameters stored\")\n",
    "         \n",
    "        self.model = create_empty_model(self.model_class,  parameters = model_parameters , device = self.device)\n",
    "        self.loss_function = create_loss_function(self.loss_function_name)\n",
    "        self.optimizer = create_optimizer(self.model ,self.optimizer_name, self.optimizer_params)\n",
    "        print(f\"model type initialized: {self.model_class}  \") \n",
    "        print(f\"Optimizer: {self.optimizer_name}\") \n",
    "        print(f\"loss function: {self.loss_function_name}\") \n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    def initiate_training(self, epochs,  train_dataloader, test_dataloader):\n",
    "        \"\"\"\n",
    "        This is the main training function. Takes a number of epochs, training dataloader, and test dataloader and initiates the actual trainig.\n",
    "        When training data is also collected about how the model performs. \n",
    "        \"\"\"\n",
    "        \n",
    "        for t in range(epochs): # train for epochs number of iterations\n",
    "           \n",
    "            epoch_stats = {\"epoch\": self.num_epochs +1}\n",
    "            print(f\"Epoch count {self.num_epochs +1}\\n----------------------------------\")\n",
    "            start_time = time.time()  # Start timer\n",
    "            self.train(train_dataloader)\n",
    "            end_time = time.time()  # End timer\n",
    "            result_time = end_time - start_time\n",
    "            print(f\"Execution time of epoch: {result_time:.2f} seconds\")\n",
    "            epoch_stats[\"time\"] = result_time\n",
    "\n",
    "            match self.model_class:\n",
    "                case \"SimpleAutoencoder\":\n",
    "                    epoch_stats[\"loss\"], epoch_stats[\"test_originals\"] , epoch_stats[\"test_created\"] = self.test_autoencoder(test_dataloader)#output \n",
    "                case _:\n",
    "                    epoch_stats[\"accuracy\"] , epoch_stats[\"loss\"] , epoch_stats[\"list_of_fails\"]=  self.test(test_dataloader)\n",
    "            \n",
    "\n",
    "            self.num_epochs += 1 # With a succesful training add one more epoch to the total\n",
    "            self.stats.append(epoch_stats) # Store metadata of the training in the stats member of the class\n",
    "        print(\"Done!\")\n",
    "        self.task_complete_alert()\n",
    "        \n",
    "        return self.stats #If the person doing the training wants to access the stats they can easily use this return\n",
    "\n",
    "    def initiate_training_shape(self, epochs,  train_dataloader, test_dataloader):\n",
    "        \"\"\"\n",
    "        This is the main training function. Takes a number of epochs, training dataloader, and test dataloader and initiates the actual trainig.\n",
    "        When training data is also collected about how the model performs. \n",
    "        \"\"\"\n",
    "        \n",
    "        for t in range(epochs): # train for epochs number of iterations\n",
    "           \n",
    "            epoch_stats = {\"epoch\": self.num_epochs +1}\n",
    "            print(f\"Epoch count {self.num_epochs +1}\\n----------------------------------\")\n",
    "            start_time = time.time()  # Start timer\n",
    "            self.train(train_dataloader)\n",
    "            end_time = time.time()  # End timer\n",
    "            result_time = end_time - start_time\n",
    "            print(f\"Execution time of epoch: {result_time:.2f} seconds\")\n",
    "            epoch_stats[\"time\"] = result_time\n",
    "\n",
    "            match self.model_class:\n",
    "                case \"SimpleAutoencoder\":\n",
    "                    epoch_stats[\"loss\"], epoch_stats[\"test_originals\"] , epoch_stats[\"test_created\"] = self.test_autoencoder(test_dataloader)#output \n",
    "                case _:\n",
    "                    epoch_stats[\"accuracy\"] , epoch_stats[\"loss\"] , epoch_stats[\"list_of_fails\"]=  self.test(test_dataloader)\n",
    "            \n",
    "\n",
    "            self.num_epochs += 1 # With a succesful training add one more epoch to the total\n",
    "            self.stats.append(epoch_stats) # Store metadata of the training in the stats member of the class\n",
    "        print(\"Done!\")\n",
    "        self.task_complete_alert()\n",
    "        \n",
    "        return self.stats #If the person doing the training wants to access the stats they can easily use this return\n",
    "    \n",
    "    \n",
    "    def train(self, train_dataloader):\n",
    "        \n",
    "        size = len(train_dataloader.dataset)\n",
    "        self.model.train() # set model to trainig mode\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device) # set the data to the appropriate device\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = self.model(X)\n",
    "            \n",
    "            match self.model_class:\n",
    "                case \"SimpleAutoencoder\":\n",
    "                    loss = self.loss_function(pred, X)\n",
    "                case _:\n",
    "                    loss = self.loss_function(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            if batch % 100 == 0:\n",
    "                loss_val, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        list_of_fails = []\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        self.model.eval() # set model to evaluation mode\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(test_dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                test_loss += self.loss_function(pred, y).item()\n",
    "\n",
    "                # Get predicted labels to create a mask of successes\n",
    "                predicted_labels = pred.argmax(1)\n",
    "                correct_mask = predicted_labels == y\n",
    "                correct += correct_mask.type(torch.float).sum().item()\n",
    "\n",
    "                #Produce a list of all fails\n",
    "                indices_of_failed_pred = (~correct_mask).nonzero(as_tuple=True)[0]\n",
    "                list_of_fails.extend([\n",
    "                {\n",
    "                    \"index\": batch * len(y) + j.item(),  # Unique index based on batch\n",
    "                    \"predic\": predicted_labels[j].item(),\n",
    "                    \"actual\": y[j].item(),\n",
    "                }\n",
    "                for j in indices_of_failed_pred\n",
    "                ])\n",
    "                \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error of epoch {self.num_epochs +1}: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        return [correct, test_loss, list_of_fails]\n",
    "\n",
    "    def test_autoencoder(self, test_dataloader):\n",
    "        \"\"\"\n",
    "        Special test case for autoencoders. We have to compare the images to images, and can visually inspect how the model progresses.\n",
    "        \"\"\"\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        self.model.eval() # set model to evaluation mode\n",
    "        test_loss = 0\n",
    "        original_images, genereated_images = [] , []\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(test_dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                test_loss += self.loss_function(pred, X).item()\n",
    "            X, y = next(iter(test_dataloader))  # Get the first batch\n",
    "            X_samples = X[:10]\n",
    "            for X in X_samples: #Display comparision of original and prediciton of the the first ten images\n",
    "                pred = self.model(X)\n",
    "                original_images.append(X) \n",
    "                genereated_images.append(pred) \n",
    "        \n",
    "        test_dataloader = iter(test_dataloader) #Reset the dataloader, we want the same comarisons each time\n",
    "        test_loss /= num_batches\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Test Error of epoch {self.num_epochs +1}: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "        display_comparissions_autoencoder(original_images, genereated_images)\n",
    "        return [test_loss, original_images, genereated_images]\n",
    "        \n",
    "    \n",
    "    def print_arcitecture(self):\n",
    "        print (self.model)\n",
    "\n",
    "    def task_complete_alert(self):\n",
    "        \"\"\"Browser notification 'Task completed!'\"\"\"\n",
    "        \n",
    "        display(Javascript('alert(\"Task completed!\")'))\n",
    "        return None\n",
    "\n",
    "    def get_max_accuarcy(self):\n",
    "        return max(self.stats, key=lambda x: x[\"accuracy\"])\n",
    "\n",
    "\n",
    "    def print_time_spent(self):\n",
    "        \"\"\"\n",
    "        gives a description of time spent to console and returns the total time spent training\n",
    "        \"\"\"\n",
    "        time = sum(t[\"time\"] for t in self.stats)\n",
    "        epochs = len(self.stats)\n",
    "        if (epochs == 0): \n",
    "            print(\"Model has not trained yet!\") \n",
    "        else:\n",
    "            minutes, seconds = divmod(time, 60)\n",
    "            print(f\"In {epochs} epochs you have trained for a total of {round(minutes)} minutes and {round(seconds)} seconds!\\nAn average of {(time/epochs):>0.2f} seconds per epoch!\")\n",
    "        return time\n",
    "\n",
    "\n",
    "    def return_time_spent(self):\n",
    "        return sum(t[\"time\"] for t in self.stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f26793-532a-4065-80aa-4542b929775d",
   "metadata": {},
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f645d290-3a46-426c-8d76-b16aede5bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_manager(mm, save_token , make_read_only = True , path = \"models\\\\\"):\n",
    "    \"\"\"Takes a modelmanager and a save_token and saves a model to disk. By default it makes the file read only, bypass with make_read_only = False\"\"\"\n",
    "    save_name = path + mm.model_class + save_token + \".pth\"\n",
    "    torch.save({'model_class': mm.model_class,\n",
    "    'model_state_dict': mm.model.state_dict(),\n",
    "    'model_parameters': mm.model_parameters,\n",
    "    'optimizer_state_dict': mm.optimizer.state_dict(),\n",
    "    'loss_function_name': mm.loss_function_name,\n",
    "    'optimizer_name': mm.optimizer_name,\n",
    "    'optimizer_params': mm.optimizer_params,\n",
    "    'hidden_layers': mm.hidden_layers,\n",
    "    'layer_sizes': mm.layer_sizes,\n",
    "    'singel_outputs': mm.singel_outputs,\n",
    "    'stats': mm.stats}, \n",
    "    save_name)\n",
    "    if make_read_only:\n",
    "        make_file_read_only(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f94744-49a1-4fcd-8e5b-bf352d124bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_manager(save_name , path = \"models\\\\\"):\n",
    "    loaded_model = torch.load( path + save_name)\n",
    "    NMM =   model_manager( model_class = loaded_model['model_class'] ,   # New model manager\n",
    "                loss_function_name = loaded_model['loss_function_name'] ,\n",
    "                optimizer_name = loaded_model['optimizer_name'],\n",
    "                optimizer_params = loaded_model['optimizer_params'],\n",
    "                model_parameters = loaded_model['model_parameters'],\n",
    "                hidden_layers = loaded_model['hidden_layers'],\n",
    "                layer_sizes = loaded_model['layer_sizes'],\n",
    "                singel_outputs = loaded_model['singel_outputs'],\n",
    "                stats = loaded_model['stats'])\n",
    "    NMM.model.load_state_dict(loaded_model[\"model_state_dict\"])\n",
    "    NMM.optimizer.load_state_dict(loaded_model['optimizer_state_dict'])\n",
    "    print(\"Load complete\")\n",
    "    return NMM\n",
    "   \n",
    "# model_class, loss_function_name ,optimizer_name,optimizer_params , model_parameters = None,  \n",
    "#                  hidden_layers = 0, layer_sizes = None, singel_outputs = False, stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b2bd8e-1424-4ce6-bedc-4107221b50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stats(save_name , path = \"models\\\\\"):\n",
    "    return torch.load(path + save_name, map_location='cpu')['stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73fc3a7-5007-432f-a824-cac3fdcd354e",
   "metadata": {},
   "source": [
    "## Data Managment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c4bce11-f137-4627-99dd-94bee25e89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training. Excellent little code snippet that checks what form of acceleration is available. Code from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "def get_device():\n",
    "    \"\"\"Get cpu, gpu or mps device for training. Excellent little code snippet that checks what form of acceleration is available. Code from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\"\"\"\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\" )\n",
    "    print(f\"Using {device} device\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ea6bde-0686-4e2b-bd53-c5ce45e370c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayTensorAsImage(tensor):\n",
    "    \"\"\"Takes a two dimentional tensor array and displays it with matplotlib.pyplot. It does not change the original data\"\"\"\n",
    "    \n",
    "    # Clone data and ensure the tensor is on CPU for matplotlib to function properly\n",
    "    display_data = tensor.clone().to(\"cpu\")\n",
    "\n",
    "    # Squeeze extra dimensions if needed on the copied tensor\n",
    "    if display_data.dim() == 3 and display_data.size(0) == 1:  # Shape (1, 28, 28)\n",
    "        display_data = display_data.squeeze(0)\n",
    "    elif display_data.dim() == 3 and display_data.size(-1) == 1:  # Shape (28, 28, 1)\n",
    "        display_data = display_data.squeeze(-1)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(display_data, cmap=\"gray\")\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190045df-6f01-4237-914e-1ea8944611bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from torchvision.datasets.\n",
    "def get_MNIST_train_data(_transform):\n",
    "    \"\"\"Returns the training data 60000 MNIST images transformed with _transform parameter\"\"\"\n",
    "    return datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform= _transform,\n",
    "    )\n",
    "\n",
    "# Download test data from open torchvision.datasets.\n",
    "def get_MNIST_test_data(_transform):\n",
    "    \"\"\"Returns the test data 10000 MNIST images transformed with _transform parameter\"\"\"\n",
    "    return datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=_transform,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2e07b963-58b5-446e-9d1b-364b8d5aaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting raw data for basic shape based neural network\n",
    "def get_raw_MNIST_train_data():\n",
    "    \"\"\" \n",
    "        datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform= transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "    \"\"\"\n",
    "    return get_MNIST_train_data( get_raw_MNIST_transform())\n",
    "\n",
    "\n",
    "def get_raw_MNIST_test_data():\n",
    "    \"\"\" \n",
    "        datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform= transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            ])\n",
    "    \"\"\"   \n",
    "    return get_MNIST_test_data( get_raw_MNIST_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52ffb2f-b451-489c-9385-6ffba4452edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_MNIST_training_transform():\n",
    "    \"\"\"transforms.RandomRotation(10),  # Randomly rotate by up to 10 degrees\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,)) # Normalize with mean and standard deviation, already known for MNIST dataset\"\"\"\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.RandomRotation(10),  # Randomly rotate by up to 10 degrees\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean and standard deviation, already known for MNIST dataset\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4c4d8d0-dfee-44df-87dc-47d058753ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_MNIST_autoencoder_training_transform():\n",
    "    \"\"\"transforms.RandomRotation(10),  # Randomly rotate by up to 10 degrees\n",
    "        transforms.ToTensor(),  # Convert to tensor\"\"\"\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.RandomRotation(10),  # Randomly rotate by up to 10 degrees\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67bd6e5-a5e1-4564-acf8-f49bc296e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_MNIST_test_transform():\n",
    "    \"\"\" transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,)) # Normalize with mean and standard deviation, already known for MNIST dataset\"\"\"\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean and standard deviation, already known for MNIST dataset\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bd7e924-ef6d-4a24-b32b-892f9ac121ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_MNIST_autoencoder_test_transform():\n",
    "    \"\"\" transforms.ToTensor(),  # Convert to tensor\"\"\"\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca76f59-35ce-452c-9e06-e23d6636a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_MNIST_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c01a0264-9333-44a6-a002-a79162344529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_batch_size_64():\n",
    "    \"\"\"returns 64\"\"\"\n",
    "    return 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e0a502-1529-4ef5-9ac0-8b467eb14b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataloader(data , _batch_size, shuffle = True , collate_fn = None):\n",
    "    \"\"\" Takes basic data and batch size as (_batch_size) returns a dataloader made with the torch.utils.data.DataLoader function\"\"\"\n",
    "    if (collate_fn == None):\n",
    "        return torch.utils.data.DataLoader(data, batch_size=_batch_size, shuffle = shuffle)\n",
    "    else: \n",
    "        return torch.utils.data.DataLoader(data, batch_size=_batch_size, shuffle = shuffle, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b69b6bac-ee15-45e6-a78b-2695648e85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make file non writeable\n",
    "def make_file_read_only(filename , path = \"\"):\n",
    "    \"\"\" \n",
    "    Function takes a filename and makes the corresponding file read-only! By deault has not path data added, \n",
    "    but path can be added with argument \"path\" \n",
    "    \"\"\"\n",
    "    os.chmod(path+filename, stat.S_IREAD|stat.S_IRGRP|stat.S_IROTH)\n",
    "    print( path+filename , \" has been made readOnly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a366d-e008-4b38-8e00-69bd8da9c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(filename , path = \"\" ):\n",
    "    \"\"\" \n",
    "    Function takes a filename and makes the corresponding file read-only! By deault has not path data added, \n",
    "    but path can be added with argument \"path\" \n",
    "    \"\"\"\n",
    "    # os.chmod(filename, stat.S_IREAD|stat.S_IRGRP|stat.S_IROTH)\n",
    "    file_path = path+filename\n",
    "    \n",
    "    try:\n",
    "        os.chmod(file_path, stat.S_IWUSR | stat.S_IREAD)\n",
    "        os.remove(file_path)\n",
    "        print(f\"Successfully deleted {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b88babe2-9c7a-4d51-9634-e67bd0af7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomNumber(n):\n",
    "    return random.randint(0, n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ab37f-b442-4cb6-bb77-867dace57102",
   "metadata": {},
   "source": [
    "## image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "274a8a6e-d47a-4216-8047-2b0cc0b6fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_3x3_to_shape(grid, threshold=0.2):\n",
    "    \"\"\"Takes a 3by3 grid and a threshold value and returns a integer between 0 and 511 for the 512 \n",
    "    binary configurations you can put a 3by3 grid in.\"\"\"\n",
    "   \n",
    "    \n",
    "    flattened_grid = np.array(grid).flatten()\n",
    "    binary_grid = [1 if value >= threshold else 0 for value in flattened_grid]\n",
    "    classification = int(''.join(str(bit) for bit in binary_grid), 2)\n",
    "\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73589007-a3a2-40b1-90cb-eba382ee6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padded(batch):\n",
    "    \"\"\"Collate function that pads variable-length feature tensors to match the longest one in the batch.\"\"\"\n",
    "    features, labels = zip(*batch)  # Separate features and labels\n",
    "\n",
    "    # Find the max length in the batch\n",
    "    max_len = max(feat.shape[0] for feat in features)\n",
    "\n",
    "    # Pad all tensors to the same length\n",
    "    padded_features = torch.stack([\n",
    "        torch.cat([feat, torch.zeros(max_len - feat.shape[0], 3)])  # Pad with zeros\n",
    "        for feat in features\n",
    "    ])\n",
    "\n",
    "    return padded_features, torch.tensor(labels, dtype=torch.long)  # Keep labels unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1d453c6-5cdf-42e4-b946-5eb01b02fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deault_threshold_value_0_2(): \n",
    "    return 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94559cf6-147e-42a9-92af-172087c84b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "357bfa44-22c2-483a-aeea-0729476d1db8",
   "metadata": {},
   "source": [
    "## Visualisations and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e48ab9d-7d91-4e64-b208-5843f2c1e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(data):\n",
    "    \"\"\"Displays image. Input to this funciton should be in the form: dataset[image_number]\"\"\"\n",
    "    imageData = data[0]\n",
    "    imageData = imageData.squeeze(0)\n",
    "    plt.imshow(imageData, cmap=\"gray\")\n",
    "    plt.axis(\"off\")  # Hides axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38fe5e07-8d41-46c3-8d74-9d355a569fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparissions_autoencoder(original, reconstructed):\n",
    "    \"\"\"Displays images for comparisson when training an autoencoder.\"\"\"\n",
    "   \n",
    "    # sample_images, _ = next(iter(torch.utils.data.DataLoader(dataset, batch_size=10)))\n",
    "    # sample_images = sample_images.to(device)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "    for i in range(len(original)):\n",
    "        axes[0, i].imshow(original[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c5b955f-e72f-422f-870e-f57c5087c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failureMatrixMNIST(listOfFails):\n",
    "    failedMatrix = [[0 for x in range(10)] for x in range(10)] \n",
    "    for elem in listOfFails:\n",
    "        failedMatrix[elem[1].int()][elem[2]] += 1\n",
    "    for row in failedMatrix:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a48dae2-ff55-4ae8-879a-57a3d6f3cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_complete_alert(self):\n",
    "    \"\"\"Browser notification 'Task completed!'\"\"\"\n",
    "    display(Javascript('alert(\"Task completed!\")'))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "251180ba-de91-4c7a-9c2a-b6705aff0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizing the Original and Reconstructed Images\n",
    "def visualize_reconstruction(model, dataset):\n",
    "    model.eval()\n",
    "    sample_images, _ = next(iter(torch.utils.data.DataLoader(dataset, batch_size=10)))\n",
    "    sample_images = sample_images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(sample_images.view(sample_images.size(0), -1)).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(sample_images[i].cpu().squeeze(), cmap=\"gray\")\n",
    "        axes[1, i].imshow(reconstructed[i].view(28, 28), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# # Show original vs reconstructed images\n",
    "# visualize_reconstruction(autoencoder, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bf95f44f-20b3-4324-a599-b9d9123cf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE the data to file:\n",
    "def save_JSON_stats(fileName, path = \"stats/\" ):\n",
    "    with open( path + fileName, \"w\") as file:\n",
    "        json.dump(stats, file, indent=2)\n",
    "    make_file_read_only( path + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7dbe7431-8cee-4437-803c-087aa2755d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load from a JSON file\n",
    "def load_JSON_stats(fileName, path = \"stats/\" ):\n",
    "    with open( path + fileName , \"r\") as file:\n",
    "        backup_stats = json.load(file)\n",
    "    return backup_stats   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a54a716c-8f74-462f-9285-f2297422bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MNISTStatsVisualizer:\n",
    "    def __init__(self):\n",
    "        self.datasets = {}  # Store multiple datasets by key\n",
    "        self.default_dataset = None  # Key for default dataset\n",
    "        self.stats_data = {}  # Store stats with cumulative total_time\n",
    "        self.stats_types = {}  # Keep track of stats format types\n",
    "    \n",
    "    def load_mnist_test_data(self, key, transform=None):\n",
    "        \"\"\"Loads MNIST test dataset and stores it under a key.\"\"\"\n",
    "        if transform is None:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.datasets[key] = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "        if self.default_dataset is None:\n",
    "            self.default_dataset = key  # Set first loaded dataset as default\n",
    "\n",
    "    def add_stats(self, key, stats, stats_type):\n",
    "        \"\"\"Adds a stats dataset and computes cumulative total_time while storing its type.\"\"\"\n",
    "        standardized_stats = [{**entry} for entry in stats]  # Simply copy entries\n",
    "        self.stats_data[key] = self._compute_total_time(standardized_stats)\n",
    "        self.stats_types[key] = stats_type  # Store the type of stats\n",
    "    \n",
    "    def _compute_total_time(self, stats):\n",
    "        \"\"\"Computes cumulative total_time for stats.\"\"\"\n",
    "        total_time = 0\n",
    "        for entry in stats:\n",
    "            total_time += entry['time']\n",
    "            entry['total_time'] = total_time\n",
    "        return stats\n",
    "    \n",
    "    def plot_accuracy(self, keys=None):\n",
    "        \"\"\"Plots accuracy over total time for multiple stat files.\"\"\"\n",
    "        keys = keys or [self.default_dataset]\n",
    "        title_parts = []\n",
    "        \n",
    "        plt.figure(figsize=(16, 8))\n",
    "        for key in keys:\n",
    "            if key not in self.stats_data or self.stats_types.get(key) == \"type_2\":\n",
    "                print(f\"Skipping accuracy plot for {key}, as it lacks accuracy data.\")\n",
    "                continue\n",
    "            \n",
    "            times = [entry['total_time'] for entry in self.stats_data[key] if 'accuracy' in entry]\n",
    "            accuracies = [entry['accuracy'] for entry in self.stats_data[key] if 'accuracy' in entry]\n",
    "            plt.plot(times, accuracies, marker='o', label=f\"Accuracy ({key})\")\n",
    "            \n",
    "            max_acc = max(accuracies)\n",
    "            title_parts.append(f\"{key}: Max Acc = {max_acc:.3f}\")\n",
    "        \n",
    "        plt.xlabel(\"Total Time (seconds)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Accuracy Over Time\\n\"  + \" | \".join(title_parts))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_loss(self, keys=None):\n",
    "        \"\"\"Plots loss over total time for multiple stat files and reports lowest loss values in the title.\"\"\"\n",
    "        keys = keys or [self.default_dataset]\n",
    "        \n",
    "        plt.figure(figsize=(16, 8))\n",
    "        title_parts = []  # To store the loss information for the title\n",
    "    \n",
    "        for key in keys:\n",
    "            if key not in self.stats_data:\n",
    "                print(\"No stats available for key\", key)\n",
    "                continue\n",
    "    \n",
    "            times = [entry['total_time'] for entry in self.stats_data[key]]\n",
    "            losses = [entry['loss'] for entry in self.stats_data[key]]\n",
    "            plt.plot(times, losses, marker='o', label=f\"Loss ({key})\", linestyle='dashed')\n",
    "    \n",
    "            # Find the minimum loss value and the corresponding time\n",
    "            min_loss = min(losses)\n",
    "            \n",
    "            # Add the minimum loss to the title\n",
    "            title_parts.append(f\"{key}: Min Loss = {min_loss:.3f}\")\n",
    "    \n",
    "        plt.xlabel(\"Total Time (seconds)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Over Time\\n\" + \" | \".join(title_parts))  # Add the loss details to the title\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def show_failed_predictions(self, stats_key, dataset_key=None, max_images=1000):\n",
    "        \"\"\"Displays failed predictions based on the epoch with the highest accuracy.\"\"\"\n",
    "        if stats_key not in self.stats_data:\n",
    "            print(\"No stats available for key\", stats_key)\n",
    "            return\n",
    "    \n",
    "        if self.stats_types.get(stats_key) == \"type_2\":\n",
    "            print(\"Failed predictions are not available for this stats type.\")\n",
    "            return\n",
    "    \n",
    "        dataset_key = dataset_key or self.default_dataset\n",
    "        if dataset_key not in self.datasets:\n",
    "            print(\"No dataset available for key\", dataset_key)\n",
    "            return\n",
    "    \n",
    "        dataset = self.datasets[dataset_key]\n",
    "    \n",
    "        # Find epoch with best accuracy\n",
    "        best_epoch = max(self.stats_data[stats_key], key=lambda x: x.get('accuracy', 0))\n",
    "        failed_predictions = best_epoch.get('list_of_fails', [])\n",
    "    \n",
    "        # Limit number of displayed failures\n",
    "        failed_predictions = failed_predictions[:max_images]\n",
    "    \n",
    "        num_images = len(failed_predictions)\n",
    "        cols = min(15, num_images)  # Set max columns to 20\n",
    "        rows = (num_images // cols) + (num_images % cols > 0)\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4.8))\n",
    "    \n",
    "        axes = axes.flatten() if num_images > 1 else [axes]\n",
    "    \n",
    "        for ax, fail in zip(axes, failed_predictions):\n",
    "            image, _ = dataset[fail['index']]\n",
    "            ax.imshow(image.squeeze(), cmap='gray')\n",
    "            ax.set_title(f\"P: {fail['predic']} / A: {fail['actual']}\", fontsize=52)  # Adjusted font size\n",
    "            ax.axis('off')\n",
    "    \n",
    "        for ax in axes[num_images:]:\n",
    "            ax.axis('off')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "  \n",
    "\n",
    "\n",
    "    def plot_double_histogram(self, stats_keys):\n",
    "        plt.figure(figsize=(16, 8))  # Enlarged figure for better comparison\n",
    "        bar_width = 0.2  # Narrower bars to fit multiple models\n",
    "        num_models = len(stats_keys)\n",
    "        indices = np.arange(10)  # Positions for digits 0-9\n",
    "    \n",
    "        title_parts = []  # Store titles with accuracy\n",
    "        colors = [  \"#1f77b4\", \"#aec7e8\", \n",
    "                    \"#ff7f0e\", \"#ffbb78\", \n",
    "                    \"#2ca02c\", \"#98df8a\",  \n",
    "                    \"#d62728\", \"#ff9896\",  \n",
    "                ]\n",
    "        \n",
    "        for i, key in enumerate(stats_keys):\n",
    "            if self.stats_types.get(key) == 2:\n",
    "                print(f\"Skipping {key} as it has 'type_2' stats, which do not have failed predictions.\")\n",
    "                continue\n",
    "    \n",
    "            stats = self.stats_data.get(key, [])\n",
    "            best_epoch = max(stats, key=lambda x: x.get('accuracy', 0))  # Find best epoch based on accuracy\n",
    "            accuracy = best_epoch.get('accuracy', 0)  # Get accuracy\n",
    "            title_parts.append(f\"{key}: {accuracy:.2%}\")  # Format as percentage\n",
    "    \n",
    "            failed_predictions = best_epoch.get('list_of_fails', [])\n",
    "    \n",
    "            actual_values = [fail['actual'] for fail in failed_predictions]\n",
    "            predicted_values = [fail['predic'] for fail in failed_predictions]\n",
    "    \n",
    "            # Count occurrences for actual and predicted values\n",
    "            actual_counts = np.bincount(actual_values, minlength=10)\n",
    "            predicted_counts = np.bincount(predicted_values, minlength=10)\n",
    "    \n",
    "            # Offset each model's bars to the right to separate them\n",
    "            offset = (i - num_models / 2) * bar_width * 2\n",
    "    \n",
    "            plt.bar(indices + offset - bar_width / 2, actual_counts, width=bar_width, label=f\"Actual - {key}\",       color= colors[i*2+0], alpha=0.7)\n",
    "            plt.bar(indices + offset + bar_width / 2, predicted_counts, width=bar_width, label=f\"Predicted - {key}\", color= colors[i*2+1], alpha=0.7)\n",
    "    \n",
    "        plt.xlabel(\"Digit\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xticks(indices, [str(i) for i in range(10)])  # Label each bar with its digit\n",
    "        plt.legend()\n",
    "        plt.title(\"Histogram of Actual vs. Predicted Values\\n\" + \" | \".join(title_parts))  # Show accuracy in title\n",
    "        plt.show()\n",
    "\n",
    "    def plot_mistake_matrix(self, stats_key):\n",
    "        \"\"\"Plots a 10x10 mistake matrix with labels and title.\"\"\"\n",
    "        if stats_key not in self.stats_data:\n",
    "            print(f\"No stats available for key: {stats_key}\")\n",
    "            return\n",
    "\n",
    "        if self.stats_types.get(stats_key) == \"type_2\":\n",
    "            print(f\"Mistake matrix is not available for stats type 'type_2' ({stats_key})\")\n",
    "            return\n",
    "\n",
    "        # Find the epoch with the highest accuracy\n",
    "        best_epoch = max(self.stats_data[stats_key], key=lambda x: x.get('accuracy', 0))\n",
    "        failed_predictions = best_epoch.get('list_of_fails', [])\n",
    "\n",
    "        # Initialize a 10x10 matrix for digit mistakes (0-9)\n",
    "        mistake_matrix = np.zeros((10, 10), dtype=int)\n",
    "\n",
    "        # Fill the matrix with failed predictions\n",
    "        for fail in failed_predictions:\n",
    "            actual = fail['actual']\n",
    "            predicted = fail['predic']\n",
    "            if actual != predicted:\n",
    "                mistake_matrix[actual, predicted] += 1  # Increment count for wrong predictions\n",
    "\n",
    "        # Plot the matrix as a heatmap\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        cax = ax.matshow(mistake_matrix, cmap=\"Blues\")\n",
    "\n",
    "        # Add color bar\n",
    "        plt.colorbar(cax)\n",
    "\n",
    "        # Set axis labels\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "        ax.set_ylabel(\"Actual Label\")\n",
    "        ax.set_title(f\"Mistake Matrix for {stats_key} (Best Epoch)\")\n",
    "\n",
    "        # Set tick labels for x and y axes\n",
    "        ax.set_xticks(np.arange(10))\n",
    "        ax.set_yticks(np.arange(10))\n",
    "        ax.set_xticklabels(np.arange(10))\n",
    "        ax.set_yticklabels(np.arange(10))\n",
    "\n",
    "        # Show values inside each cell\n",
    "        for i in range(10):\n",
    "            for j in range(10):\n",
    "                value = mistake_matrix[i, j]\n",
    "                if value > 0:  # Only show non-zero values\n",
    "                    ax.text(j, i, str(value), ha='center', va='center', color=\"black\", fontsize=10)\n",
    "\n",
    "        plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87857d19-fc44-48d4-a716-787ba86454b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "11f47105-89b9-4d9c-858f-fbcac80cc920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f29de6b8-47da-47c0-b179-135bd4b2c9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CONTRAST_THRESHOLD():\n",
    "    return  0.5\n",
    "    \n",
    "def features_to_image(features, original_size=(28, 28)):\n",
    "    if len(features) == 0:\n",
    "        return torch.zeros(original_size)\n",
    "    \n",
    "    height, width = original_size\n",
    "    center_x, center_y = width / 2 - 0.5, height / 2 - 0.5\n",
    "    \n",
    "    # Create empty image\n",
    "    image = torch.zeros(original_size)\n",
    "    \n",
    "    # Convert each feature back to pixel position\n",
    "    for feature in features:\n",
    "        contrast, pos_x, pos_y = feature\n",
    "        # Convert normalized positions back to pixel coordinates using tensor operations\n",
    "        x = torch.round(pos_x * center_x + center_x).int().item()\n",
    "        y = torch.round(pos_y * center_y + center_y).int().item()\n",
    "        \n",
    "        # Clamp to image bounds\n",
    "        x = max(0, min(width - 1, x))\n",
    "        y = max(0, min(height - 1, y))\n",
    "        \n",
    "        # Add contrast value to the pixel\n",
    "        image[y, x] = contrast\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_contrast_features(image, threshold=0.5):\n",
    "    \"\"\"Optimized version with tensor operations\"\"\"\n",
    "    if not 0 <= threshold <= 1:\n",
    "        raise ValueError(f\"Threshold must be in [0,1], got {threshold}\")\n",
    "    \n",
    "    if image.max() > 1:\n",
    "        image = image.float() / 255.0\n",
    "    \n",
    "    _, h, w = image.shape\n",
    "    if h < 2 or w < 2:\n",
    "        return torch.zeros((0, 3), dtype=torch.float32)\n",
    "    \n",
    "    center_x, center_y = (w - 1) / 2, (h - 1) / 2\n",
    "    img = image.squeeze()\n",
    "\n",
    "    # Compute contrast differences\n",
    "    right_diff = torch.abs(img[:, :-1] - img[:, 1:])\n",
    "    bottom_diff = torch.abs(img[:-1, :] - img[1:, :])\n",
    "    diag_diff = torch.abs(img[:-1, :-1] - img[1:, 1:])\n",
    "    bl_diff = torch.abs(img[:-1, 1:] - img[1:, :-1])\n",
    "    \n",
    "    feature_list = []  # Use a list for storing mini tensors\n",
    "\n",
    "    # Apply masks & collect features\n",
    "    for diff, shift_x, shift_y in [(right_diff, 0, 0), (bottom_diff, 0, 0), \n",
    "                                   (diag_diff, 0, 0), (bl_diff, 1, 0)]:\n",
    "        mask = diff > threshold\n",
    "        y_coords, x_coords = mask.nonzero(as_tuple=True)\n",
    "        if y_coords.numel() > 0:  # Only if there are valid points\n",
    "            feature_list.append(torch.stack([\n",
    "                diff[y_coords, x_coords],  \n",
    "                (x_coords + shift_x - center_x) / center_x,  \n",
    "                (y_coords + shift_y - center_y) / center_y  \n",
    "            ], dim=1))\n",
    "\n",
    "    if not feature_list:\n",
    "        return torch.zeros((0, 3), dtype=torch.float32)\n",
    "\n",
    "    return torch.cat(feature_list, dim=0)  # More efficient than appending lists\n",
    "\n",
    "# Function to find lines (including diagonal and anti-diagonal) and add scalers\n",
    "def find_lines_and_scalers(features, original_size=(28, 28)):\n",
    "    if len(features) == 0:\n",
    "        return torch.zeros((0, 3), dtype=torch.float32)\n",
    "    \n",
    "    # Sort features based on positions (x, y)\n",
    "    features = features.sort(dim=0)[0]\n",
    "\n",
    "    lines = []  # Will hold the new grouped features\n",
    "    current_line = []\n",
    "    last_position = None\n",
    "    line_type = None  # Track the type of symmetry for the line (horizontal, vertical, diagonal, etc.)\n",
    "    \n",
    "    # Process each contrast point and group into lines\n",
    "    for i, feature in enumerate(features):\n",
    "        contrast, pos_x, pos_y = feature\n",
    "        \n",
    "        # Check if the contrast is in the same line (based on position)\n",
    "        if last_position is not None:\n",
    "            # Check if points are connected horizontally, vertically, or diagonally\n",
    "            if (abs(pos_x - last_position[0]) <= 1 and pos_y == last_position[1]):  # Horizontal\n",
    "                if line_type != 'horizontal':  # A new line type starts\n",
    "                    if current_line:\n",
    "                        lines.append(current_line)  # Store the previous line\n",
    "                    current_line = [feature]  # Start a new line\n",
    "                    line_type = 'horizontal'\n",
    "            elif (abs(pos_y - last_position[1]) <= 1 and pos_x == last_position[0]):  # Vertical\n",
    "                if line_type != 'vertical':\n",
    "                    if current_line:\n",
    "                        lines.append(current_line)\n",
    "                    current_line = [feature]\n",
    "                    line_type = 'vertical'\n",
    "            elif (abs(pos_x - last_position[0]) == abs(pos_y - last_position[1]) and  # Diagonal\n",
    "                  (pos_x > last_position[0] and pos_y > last_position[1]) or\n",
    "                  (pos_x < last_position[0] and pos_y < last_position[1])):\n",
    "                if line_type != 'diagonal':\n",
    "                    if current_line:\n",
    "                        lines.append(current_line)\n",
    "                    current_line = [feature]\n",
    "                    line_type = 'diagonal'\n",
    "            elif (abs(pos_x - last_position[0]) == abs(pos_y - last_position[1]) and  # Anti-diagonal\n",
    "                  (pos_x > last_position[0] and pos_y < last_position[1]) or\n",
    "                  (pos_x < last_position[0] and pos_y > last_position[1])):\n",
    "                if line_type != 'anti-diagonal':\n",
    "                    if current_line:\n",
    "                        lines.append(current_line)\n",
    "                    current_line = [feature]\n",
    "                    line_type = 'anti-diagonal'\n",
    "\n",
    "        else:\n",
    "            current_line = [feature]  # First element in line\n",
    "            line_type = 'horizontal'  # Default to horizontal, can be adjusted as needed\n",
    "        \n",
    "        last_position = (pos_x, pos_y)  # Update last position\n",
    "    \n",
    "    # Append the last line\n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "\n",
    "    # Now, for each line, calculate the scaler (center of the line)\n",
    "    new_features = []\n",
    "    for line in lines:\n",
    "        # For simplicity, let's calculate the center of the line (mean position)\n",
    "        contrast_values = [f[0] for f in line]\n",
    "        positions_x = [f[1] for f in line]\n",
    "        positions_y = [f[2] for f in line]\n",
    "        \n",
    "        # Calculate the center of the line\n",
    "        center_x = torch.mean(torch.tensor(positions_x))\n",
    "        center_y = torch.mean(torch.tensor(positions_y))\n",
    "        \n",
    "        # Calculate line length\n",
    "        line_length = len(line)\n",
    "        scaler = min(1.0, line_length / original_size[0])  # Maximum scaler is 1\n",
    "        \n",
    "        # Create new feature with contrast and normalized position\n",
    "        new_features.append(torch.tensor([sum(contrast_values) / len(contrast_values), center_x, center_y, scaler]))\n",
    "\n",
    "    return torch.stack(new_features)  # Return new feature tensor\n",
    "\n",
    "# Updated dataset converter that includes labels\n",
    "def convert_and_optimize_dataset(dataset, optimize = True):\n",
    "    \"\"\"\n",
    "    Convert entire dataset and apply line detection and scaler addition, keeping labels.\n",
    "    Optimizer find_lines_and_scalers does not work yet\n",
    "    \n",
    "    takes:\n",
    "        dataset: PyTorch dataset of images and labels (e.g., MNIST)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of optimized contrast feature tensors, along with their corresponding labels\n",
    "    \"\"\"\n",
    "    optimized_data = []\n",
    "    for img, label in dataset:  # Now including labels\n",
    "        \n",
    "        features = extract_contrast_features(img)\n",
    "        if (optimize):\n",
    "            optimized_features = find_lines_and_scalers(features)\n",
    "        else:\n",
    "            optimized_features = features\n",
    "        optimized_data.append((optimized_features, label))  # Store both features and labels\n",
    "    \n",
    "    return optimized_data\n",
    "\n",
    "\n",
    "def compare_images(img,  reconstructed , text):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\" + text)\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Reconstructed from Contrast Features\")\n",
    "    plt.imshow(reconstructed, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
