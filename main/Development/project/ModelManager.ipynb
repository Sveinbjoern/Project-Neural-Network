{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31192263-62a0-40c5-89cc-4321185329bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45c023-ad06-43ab-8a09-ad401855d0a8",
   "metadata": {},
   "source": [
    "# Model manager, contains code that is used multiple times in the project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ac6d4-0d23-41da-afd6-567c5a371b78",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70164397-7a12-420a-97c3-9e9f928b6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10250d1a-4e3d-42ef-ba12-1f1af676f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e21dfb0-af50-432d-aa9c-d65c6c49a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58123c0-a225-410b-9cb2-0bfe1417b9b7",
   "metadata": {},
   "source": [
    "### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48aa3bc1-3a5d-45a7-8ac7-64253d1d3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyModels import create_empty_model\n",
    "from MyModels import create_loss_function\n",
    "from MyModels import create_optimizer\n",
    "import HelperFunctions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac271f2-8574-43e0-88af-bcf97ff06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_manager():\n",
    "    \"\"\"\n",
    "    Model managers gathers everything with testing and training in one class!\n",
    "    To create a new model manager you need: model_class, loss_function_name ,optimizer_name,optimizer_params. \n",
    "    Has default values for: model_parameters = {},  hidden_layers = 0, layer_sizes = [], singel_outputs = False, \n",
    "    stats = []\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_class, loss_function_name ,optimizer_name,optimizer_params , model_parameters = None,  \n",
    "                 hidden_layers = 0, layer_sizes = None, singel_outputs = False, stats = None):\n",
    "        \"\"\" Needs: model_class, loss_function_name ,optimizer_name,optimizer_params. \n",
    "    Has default values for: model_parameters = {},  hidden_layers = 0, layer_sizes = [], singel_outputs = False,  stats = [] \"\"\"\n",
    "        # avoid issues with parameters being created at initialization by exlicitly making the None then create emrty mutables\n",
    "        if model_parameters == None: model_parameters = {}\n",
    "        if stats == None:            stats = []\n",
    "        if layer_sizes == None:      layer_sizes = []\n",
    "        \n",
    "        \n",
    "        # Set parameters\n",
    "        self.device = get_device()\n",
    "        self.model_class = model_class\n",
    "        self.model_parameters = model_parameters\n",
    "        self.loss_function_name = loss_function_name  \n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.singel_outputs = singel_outputs\n",
    "        self.num_epochs = len(stats)\n",
    "        self.stats = stats\n",
    "        \n",
    "        print(\"parameters stored\")\n",
    "         \n",
    "        self.model = create_empty_model(self.model_class,  parameters = model_parameters , device = self.device)\n",
    "        self.loss_function = create_loss_function(self.loss_function_name)\n",
    "        self.optimizer = create_optimizer(self.model ,self.optimizer_name, self.optimizer_params)\n",
    "        print(f\"model type initialized: {self.model_class}  \") \n",
    "        print(f\"Optimizer: {self.optimizer_name}\") \n",
    "        print(f\"loss function: {self.loss_function_name}\") \n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    def initiate_training(self, epochs,  train_dataloader, test_dataloader):\n",
    "        \"\"\"This is the main training function. Takes a number of epochs, training dataloader, and test dataloader and initiates the actual trainig.\n",
    "    When training data is also collected about how the model performs. \"\"\"\n",
    "        \n",
    "        for t in range(epochs): # train for epochs number of iterations\n",
    "           \n",
    "            epoch_stats = {\"epoch\": self.num_epochs +1}\n",
    "            print(f\"Epoch count {self.num_epochs +1}\\n----------------------------------\")\n",
    "            start_time = time.time()  # Start timer\n",
    "            self.train(train_dataloader)\n",
    "            end_time = time.time()  # End timer\n",
    "            result_time = end_time - start_time\n",
    "            print(f\"Execution time of epoch: {result_time:.2f} seconds\")\n",
    "            epoch_stats[\"time\"] = result_time\n",
    "\n",
    "            match self.model_class:\n",
    "                case \"SimpleAutoencoder\":\n",
    "                    epoch_stats[\"loss\"], epoch_stats[\"test_originals\"] , epoch_stats[\"test_created\"] = self.test_autoencoder(test_dataloader)#output \n",
    "                case _:\n",
    "                    epoch_stats[\"accuracy\"] , epoch_stats[\"loss\"] , epoch_stats[\"list_of_fails\"]=  self.test(test_dataloader)\n",
    "            \n",
    "\n",
    "            self.num_epochs += 1 # With a succesful training add one more epoch to the total\n",
    "            self.stats.append(epoch_stats) # Store metadata of the training in the stats member of the class\n",
    "        print(\"Done!\")\n",
    "        self.task_complete_alert()\n",
    "        \n",
    "        return self.stats #If the person doing the training wants to access the stats they can easily use this return\n",
    "\n",
    "    def initiate_training_shape(self, epochs,  train_dataloader, test_dataloader):\n",
    "        \"\"\"This is the main training function. Takes a number of epochs, training dataloader, and test dataloader and initiates the actual trainig.\n",
    "    When training data is also collected about how the model performs. \"\"\"\n",
    "        \n",
    "        for t in range(epochs): # train for epochs number of iterations\n",
    "           \n",
    "            epoch_stats = {\"epoch\": self.num_epochs +1}\n",
    "            print(f\"Epoch count {self.num_epochs +1}\\n----------------------------------\")\n",
    "            start_time = time.time()  # Start timer\n",
    "            self.train(train_dataloader)\n",
    "            end_time = time.time()  # End timer\n",
    "            result_time = end_time - start_time\n",
    "            print(f\"Execution time of epoch: {result_time:.2f} seconds\")\n",
    "            epoch_stats[\"time\"] = result_time\n",
    "\n",
    "            match self.model_class:\n",
    "                case \"SimpleAutoencoder\":\n",
    "                    epoch_stats[\"loss\"], epoch_stats[\"test_originals\"] , epoch_stats[\"test_created\"] = self.test_autoencoder(test_dataloader)#output \n",
    "                case _:\n",
    "                    epoch_stats[\"accuracy\"] , epoch_stats[\"loss\"] , epoch_stats[\"list_of_fails\"]=  self.test(test_dataloader)\n",
    "            \n",
    "\n",
    "            self.num_epochs += 1 # With a succesful training add one more epoch to the total\n",
    "            self.stats.append(epoch_stats) # Store metadata of the training in the stats member of the class\n",
    "        print(\"Done!\")\n",
    "        self.task_complete_alert()\n",
    "        \n",
    "        return self.stats #If the person doing the training wants to access the stats they can easily use this return\n",
    "    \n",
    "    \n",
    "    def train(self, train_dataloader):\n",
    "        \n",
    "        size = len(train_dataloader.dataset)\n",
    "        self.model.train() # set model to trainig mode\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device) # set the data to the appropriate device\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = self.model(X)\n",
    "            \n",
    "            match self.model_class:\n",
    "                case \"SimpleAutoencoder\":\n",
    "                    loss = self.loss_function(pred, X)\n",
    "                case _:\n",
    "                    loss = self.loss_function(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            if batch % 100 == 0:\n",
    "                loss_val, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        list_of_fails = []\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        self.model.eval() # set model to evaluation mode\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(test_dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                test_loss += self.loss_function(pred, y).item()\n",
    "\n",
    "                # Get predicted labels to create a mask of successes\n",
    "                predicted_labels = pred.argmax(1)\n",
    "                correct_mask = predicted_labels == y\n",
    "                correct += correct_mask.type(torch.float).sum().item()\n",
    "\n",
    "                #Produce a list of all fails\n",
    "                indices_of_failed_pred = (~correct_mask).nonzero(as_tuple=True)[0]\n",
    "                list_of_fails.extend([\n",
    "                {\n",
    "                    \"index\": batch * len(y) + j.item(),  # Unique index based on batch\n",
    "                    \"predic\": predicted_labels[j].item(),\n",
    "                    \"actual\": y[j].item(),\n",
    "                }\n",
    "                for j in indices_of_failed_pred\n",
    "                ])\n",
    "                \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error of epoch {self.num_epochs +1}: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        return [correct, test_loss, list_of_fails]\n",
    "\n",
    "    def test_autoencoder(self, test_dataloader):\n",
    "        \"\"\"Special test case for autoencoders. We have to compare the images to images, and can visually inspect how the model progresses.\"\"\"\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        self.model.eval() # set model to evaluation mode\n",
    "        test_loss = 0\n",
    "        original_images, genereated_images = [] , []\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(test_dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                test_loss += self.loss_function(pred, X).item()\n",
    "            X, y = next(iter(test_dataloader))  # Get the first batch\n",
    "            X_samples = X[:10]\n",
    "            for X in X_samples: #Display comparision of original and prediciton of the the first ten images\n",
    "                pred = self.model(X)\n",
    "                original_images.append(X) \n",
    "                genereated_images.append(pred) \n",
    "        \n",
    "        test_dataloader = iter(test_dataloader) #Reset the dataloader, we want the same comarisons each time\n",
    "        test_loss /= num_batches\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Test Error of epoch {self.num_epochs +1}: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "        display_comparissions_autoencoder(original_images, genereated_images)\n",
    "        return [test_loss, original_images, genereated_images]\n",
    "        \n",
    "    \n",
    "    def print_arcitecture(self):\n",
    "        print (self.model)\n",
    "\n",
    "    def task_complete_alert(self):\n",
    "        \"\"\"Browser notification 'Task completed!'\"\"\"\n",
    "        \n",
    "        display(Javascript('alert(\"Task completed!\")'))\n",
    "        return None\n",
    "\n",
    "    def get_max_accuarcy(self):\n",
    "        return max(self.stats, key=lambda x: x[\"accuracy\"])\n",
    "\n",
    "\n",
    "    def print_time_spent(self):\n",
    "        \"\"\"gives a description of time spent to console and returns the total time spent training\"\"\"\n",
    "        time = sum(t[\"time\"] for t in self.stats)\n",
    "        epochs = len(self.stats)\n",
    "        if (epochs == 0): \n",
    "            print(\"Model has not trained yet!\") \n",
    "        else:\n",
    "            minutes, seconds = divmod(time, 60)\n",
    "            print(f\"In {epochs} epochs you have trained for a total of {round(minutes)} minutes and {round(seconds)} seconds!\\nAn average of {(time/epochs):>0.2f} seconds per epoch!\")\n",
    "        return time\n",
    "\n",
    "\n",
    "    def return_time_spent(self):\n",
    "        return sum(t[\"time\"] for t in self.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83f0c2-945f-4234-a7f2-8d9e02ece1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
