{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6714494-86fb-4743-aa2a-1a1f0d9657a0",
   "metadata": {},
   "source": [
    "# MyModels, contains the models and code for creating and recreating model arcitecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77822d4c-b4ff-4a0f-9656-433cc5de4571",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cce2115-cb9f-4ce7-9cdd-f5ccc47a0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03972b7-a1a6-47ad-b0fe-fc84470e8265",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb8052d-5bdf-4000-8b2c-04992fc7f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simnple neural network that tries to learn with four hidden layers.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd7e094-603d-46f0-8ba4-18f894d0c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simnple shape based neural network that tries to learn with four hidden layers.\n",
    "class ShapeBasedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(ShapeBasedNeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f8aad4-e1b9-480c-aa7c-cd6c38ed2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simnple convolutional neural network.\n",
    "class CONVNet(nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(CONVNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  \n",
    "        self.fc2 = nn.Linear(120, 84) \n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  \n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # #print(x.shape) \n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # #print(f\"After conv1 and pool: {x.shape}\")\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # #print(f\"After conv2 and pool: {x.shape}\")\n",
    "        # x = x.view(-1, 16 * 4 * 4)\n",
    "        # #print(f\"After flattening: {x.shape}\")\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2d5c7a-4124-47da-aa47-2bc0965176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simnple neural network that tries to learn with coustom hidden layers. Not tested\n",
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(CustomNeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.array_of_layers = []\n",
    "        for inputs, outputs in parameters.layers[:-1]:\n",
    "            array_of_layers.append(nn.Linear(inputs, outputs))\n",
    "            array_of_layers.append(nn.ReLU())\n",
    "        array_of_layers.append( nn.Linear(paramenters.layer_sizes[-1][0],paramenters.layer_sizes[-1][1]))\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for layer in array_of_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b850a0f-60c9-40af-a27d-5234cf286008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An autoencoder network to be able to reacreate \n",
    "class SimpleAutoencoder(nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.encode_only = False\n",
    "        \n",
    "        # first the encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32), # Compressed feature vector\n",
    "        )\n",
    "\n",
    "        # then the decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid(),  # Outputs pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if (self.encode_only):\n",
    "            with torch.no_grad():\n",
    "                encoded = self.encoder(x)\n",
    "            return encoded \n",
    "        else:\n",
    "            encoded = self.encoder(x)\n",
    "            decoded =  self.decoder(encoded)\n",
    "            return decoded.view(-1, 1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cece59a9-bcb3-4f94-a42c-dc9f5b65c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier that takes encoded data \n",
    "class ClassifierOfEncodedData(nn.Module):\n",
    "    def __init__(self,  parameters):\n",
    "        super(ClassifierOfEncodedData, self).__init__()\n",
    "        self.encoder = parameters[\"encoder\"]  # Pretrained encoder from Autoencoder (frozen)\n",
    "        self.encoder.encode_only = True\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10) # 10 output classes for digits 0-9\n",
    "           \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x) # The model takes care of no_grad\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1456089-dc3f-4551-a1df-f3f6c2973a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier that uses the tranformer model\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=7, in_channels=1, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Linear projection of flattened patches\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # (batch, embed_dim, num_patches^(1/2), num_patches^(1/2))\n",
    "        x = x.flatten(2)  # Flatten spatial dimensions\n",
    "        x = x.transpose(1, 2)  # (batch, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        #linear transformations for q, k and v\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        K = self.k_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        V = self.v_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        x = torch.matmul(attn_weights, V)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.embed_dim\n",
    "        )\n",
    "        x = self.out_linear(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))  # Add & Norm\n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))   # Add & Norm\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self,  parameters, img_size=28, patch_size=28, in_channels=1, num_classes=10, \n",
    "                 embed_dim=64, num_heads=4, depth=6, mlp_dim=128):\n",
    "        super(VisualTransformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[TransformerEncoderBlock(embed_dim, num_heads, mlp_dim) for _ in range(depth)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x) \n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x += self.pos_embed\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46b5bd41-2ea5-41b3-b7d8-55c13dc7b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier that uses the alternative tranformer model\n",
    "\n",
    "class AltPatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=7, in_channels=1, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Linear projection of flattened patches\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # (batch, embed_dim, num_patches^(1/2), num_patches^(1/2))\n",
    "        x = x.flatten(2)  # Flatten spatial dimensions\n",
    "        x = x.transpose(1, 2)  # (batch, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class AltMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        #linear transformations for q, k and v\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.q_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        K = self.k_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        V = self.v_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        x = torch.matmul(attn_weights, V)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.embed_dim\n",
    "        )\n",
    "        x = self.out_linear(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "class AltTransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = AltMultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))  # Add & Norm\n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))   # Add & Norm\n",
    "        return x\n",
    "\n",
    "\n",
    "class AltVisualTransformer(nn.Module):\n",
    "    def __init__(self,  parameters, img_size=28, patch_size=28, in_channels=1, num_classes=10, \n",
    "                 embed_dim=64, num_heads=4, depth=6, mlp_dim=128):\n",
    "        super(AltVisualTransformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = AltPatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[AltTransformerEncoderBlock(embed_dim, num_heads, mlp_dim) for _ in range(depth)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x) \n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x += self.pos_embed\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5b040a-1d00-4236-b01c-9b4dcecdee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.qkv_linear = nn.Linear(embed_dim, 3 * embed_dim)  # Combine Q, K, V\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = 1.0 / math.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.qkv_linear(x).chunk(3, dim=-1)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        return self.out_linear(torch.matmul(attn_weights, V))\n",
    "\n",
    "class ShapeTransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = ShapeSelfAttention(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))  \n",
    "        x = x + self.mlp(self.norm2(x))   \n",
    "        return x\n",
    "\n",
    "class ShapeVisualTransformer(nn.Module):\n",
    "    def __init__(self, parameter, input_dim=3, embed_dim=8, num_classes=10, depth=3, mlp_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # **Input Projection Layer**: Maps input_dim (3) → embed_dim (32)\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)  \n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[ShapeTransformerEncoderBlock(embed_dim, mlp_dim) for _ in range(depth)]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # print(\"Using forward\")\n",
    "        # print(\"batch\" , batch)\n",
    "        result = torch.zeros(8)\n",
    "        for x  in batch:\n",
    "            x = self.input_proj(x)  # Convert (batch, seq_len, 3) → (batch, seq_len, 32)\n",
    "            result += self.transformer(x)\n",
    "        return self.head(self.norm(result.mean(dim=1)))  # Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6893baf-0482-40d4-af49-b13e49bf3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeTransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), key_padding_mask=mask)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ShapeTransformer(nn.Module):\n",
    "    def __init__(self, parameter, input_dim=3, embed_dim=128, num_classes=10, depth=4, mlp_dim=64, max_features=50):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)  \n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [ShapeTransformerEncoderBlock(embed_dim, mlp_dim) for _ in range(depth)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        batch: list of feature tensors (each tensor is (num_features, 3))\n",
    "        \"\"\"\n",
    "        sampled_batch = []\n",
    "        for x in batch:\n",
    "            # Randomly sample up to `max_features`\n",
    "            if x.shape[0] > self.max_features:\n",
    "                indices = torch.randperm(x.shape[0])[:self.max_features]  # Random indices\n",
    "                x = x[indices]\n",
    "            sampled_batch.append(x)\n",
    "\n",
    "        # Project inputs to embedding space\n",
    "        batch_embedded = [self.input_proj(x) for x in sampled_batch]\n",
    "\n",
    "        # Pad sequences to max length in batch\n",
    "        padded_batch = pad_sequence(batch_embedded, batch_first=True, padding_value=0)\n",
    "        mask = (padded_batch.abs().sum(dim=-1) == 0)  # Mask padding\n",
    "\n",
    "        # Apply Transformer Encoder blocks\n",
    "        for block in self.encoder_blocks:\n",
    "            padded_batch = block(padded_batch, mask=mask)\n",
    "\n",
    "        # Global Average Pooling over valid features (ignoring padding)\n",
    "        valid_counts = (~mask).sum(dim=1, keepdim=True).float()\n",
    "        pooled = padded_batch.sum(dim=1) / valid_counts\n",
    "\n",
    "        return self.head(self.norm(pooled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3e937-9257-42ff-9294-7f5c038daf53",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18445eb9-9a3d-4050-a71a-2d5f72006ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_model(model_name , parameters= {}, device= \"cpu\"):\n",
    "    \"\"\"Takes a name of a model and the device to return a model with aritecture, but does not contain trained weights, loss function or optimizer\"\"\"\n",
    "    models_class_map = {\n",
    "        \"NeuralNetwork\": NeuralNetwork,\n",
    "        \"ShapeBasedNeuralNetwork\": ShapeBasedNeuralNetwork,\n",
    "        \"CONVNet\": CONVNet,\n",
    "        \"SimpleAutoencoder\": SimpleAutoencoder,\n",
    "        \"ClassifierOfEncodedData\": ClassifierOfEncodedData,\n",
    "        \"VisualTransformer\": VisualTransformer,\n",
    "        \"AltVisualTransformer\": AltVisualTransformer,\n",
    "        \"ShapeVisualTransformer\": ShapeVisualTransformer,\n",
    "        \"ShapeTransformer\": ShapeTransformer,\n",
    "    }\n",
    "    return models_class_map[model_name](parameters).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ea6f15e-ffc7-4670-ad32-8a451fbaf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_function(loss_function ):\n",
    "    \"\"\"Takes a name of a loss function to return a loss function\"\"\"\n",
    "    loss_functions_map = {\n",
    "    \"CrossEntropyLoss\": nn.CrossEntropyLoss,\n",
    "    \"MSELoss\": nn.MSELoss,\n",
    "    }\n",
    "    return loss_functions_map[loss_function]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0b098ce-60c9-4cfc-848e-99dda8157eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer_name, optimizer_params ):\n",
    "    \"\"\"Takes a name of a optimizer and its parmameters and to returns an opmtimizer\"\"\"\n",
    "    optimizers_map = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "        \n",
    "    }\n",
    "    return optimizers_map[optimizer_name](model.parameters(), **optimizer_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
